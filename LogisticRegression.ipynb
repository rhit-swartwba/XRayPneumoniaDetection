{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f6c553-0ef3-4299-be43-d876714461d3",
   "metadata": {},
   "source": [
    "# X-Ray Pneumonia Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28165eb5-4ced-4844-a00e-79eb1be342ac",
   "metadata": {},
   "source": [
    "### 1a) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadb8681-d104-45d1-9f65-becbc16df103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b358ea-ecba-426b-a47e-cb22ec5b65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract and flatten features using the VGG16 model\n",
    "def extract_features(data_loader, model, device):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for images, targets in data_loader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass through the VGG16 model to extract features\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Flatten the features (batch size, -1)\n",
    "            outputs_flat = outputs.view(outputs.size(0), -1)\n",
    "            \n",
    "            # Collect the flattened features and corresponding labels\n",
    "            features.append(outputs_flat.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    \n",
    "    # Concatenate the feature and label arrays\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca439d29-cbb2-4f91-b223-bbb273adebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swartwba/.conda/envs/csse416/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/swartwba/.conda/envs/csse416/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature shape for (224, 224): torch.Size([1, 512, 7, 7])\n",
      "Flattened feature shape for (224, 224): torch.Size([1, 25088])\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Remove the classifier part of VGG16 (keep only the convolutional layers)\n",
    "vgg16_features = vgg16.features  # This includes only the convolutional layers, no fully connected layers\n",
    "vgg16_features = vgg16_features.to(device)\n",
    "\n",
    "# Define the image transformations (resizing, normalization, etc.)\n",
    "img_size = (224, 224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),  # Resize images to the desired size (32x32, 64x64, 128x128)\n",
    "    transforms.ToTensor(),        # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize to match VGG16's training\n",
    "])\n",
    "\n",
    "# Test feature extraction and flatten the result\n",
    "with torch.no_grad():\n",
    "    sample_image = torch.randn(1, 3, img_size[0], img_size[1]).to(device)  # Create a dummy image of the specified size\n",
    "    features = vgg16_features(sample_image)  # Extract features using convolutional layers\n",
    "    print(f\"Original feature shape for {img_size}: {features.shape}\")\n",
    "\n",
    "    # Flatten the feature map (batch size is 1, so we flatten from index 1 onward)\n",
    "    features_flat = features.view(features.size(0), -1)  # Flatten to (num_samples, num_features)\n",
    "    print(f\"Flattened feature shape for {img_size}: {features_flat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048c7c28-cfbf-4675-9c54-cf54ffa2dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # ResNet expects 224x224 images\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalization for pre-trained models\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '../data/chest_xray'\n",
    "image_datasets = {\n",
    "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms['train']),\n",
    "    'val': datasets.ImageFolder(os.path.join(data_dir, 'val'), data_transforms['val']),\n",
    "    'test': datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False),\n",
    "    'test': DataLoader(image_datasets['test'], batch_size=32, shuffle=False)\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f346e58b-8733-4ce9-bf2d-779d4156d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels using the VGG16 model for each dataset split\n",
    "X_train, y_train = extract_features(dataloaders['train'], vgg16, device)\n",
    "X_val, y_val = extract_features(dataloaders['val'], vgg16, device)\n",
    "X_test, y_test = extract_features(dataloaders['test'], vgg16, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e66e4f-8ff4-43f8-ad3c-e43b24989a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (1000,)\n",
      "Labels shape: (5216,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the extracted features\n",
    "print(f\"Extracted features shape: {X_train.shape[1:]}\")\n",
    "print(f\"Labels shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd88093-ebfd-4f81-97d0-284544008e2d",
   "metadata": {},
   "source": [
    "### 1b) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4677c4b2-0725-4c45-b390-f9d583068598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1.3 min\n",
      "Alpha (1 / lgCV.C_): 1.0\n",
      "Training error rate: 0.008\n",
      "Testing error rate: 0.196\n",
      "Training Accuracy: 0.008\n",
      "Test Accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical computations and array manipulation\n",
    "from time import time  # For measuring performance time\n",
    "from sklearn.linear_model import LogisticRegressionCV  # Logistic Regression with cross-validation\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "\n",
    "# Define regularization hyper-parameter (inverse of alpha)\n",
    "alpha = 10**np.linspace(-10, 10, 21)\n",
    "C = 1 / alpha\n",
    "\n",
    "# Logistic Regression with Cross-Validation\n",
    "lgCV = LogisticRegressionCV(Cs=C, n_jobs=-1, max_iter=10000)\n",
    "\n",
    "# Fit logistic regressor using the flattened training data\n",
    "time_start = time()\n",
    "lgCV.fit(X_train, y_train)  # Fit on flattened training data\n",
    "time_stop = time()\n",
    "time_elapsed = time_stop - time_start\n",
    "print('Elapsed time:', round(time_elapsed / 60, 1), 'min')\n",
    "\n",
    "# Error rate calculation function\n",
    "def ErrorRate(y_true, y_pred):\n",
    "    return (y_true != y_pred).mean()\n",
    "\n",
    "# Compute training and test error rates\n",
    "print('Alpha (1 / lgCV.C_):', (1 / lgCV.C_)[0])\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = lgCV.predict(X_train)  # Predictions on flattened training data\n",
    "y_pred_test = lgCV.predict(X_test)    # Predictions on flattened test data\n",
    "\n",
    "# Error rates\n",
    "print('Training error rate:', ErrorRate(y_train, y_pred_train).round(3))\n",
    "print('Testing error rate:', ErrorRate(y_test, y_pred_test).round(3))\n",
    "print('Training Accuracy:', ErrorRate(y_train, y_pred_train).round(3))\n",
    "print('Test Accuracy:', 1 - ErrorRate(y_test, y_pred_test).round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csse416",
   "language": "python",
   "name": "csse416"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
